<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="青峰碧陋室" type="application/atom+xml">
  <meta name="baidu-site-verification" content="wqhixSGF0v">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://tengweitw.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"hide","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: './public/search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="机器学习的过程大致分为三步：1）模型假设，比如我们假设模型是线性回归，还是多项式回归，以及其阶数的选择；2）误差函数定义，比如我们假设误差函数是均方误差，还是交叉熵；3）参数求解，比如使用正规方程，还是梯度下降等。 这篇文章主要讨论模型的选择问题，下面以多项式回归为例进行说明">
<meta property="og:type" content="article">
<meta property="og:title" content="【图解例说机器学习】模型选择：偏差与方差 (Bias vs. Variance)">
<meta property="og:url" content="http://tengweitw.com/2020/04/17/[20200417]/index.html">
<meta property="og:site_name" content="青峰碧陋室">
<meta property="og:description" content="机器学习的过程大致分为三步：1）模型假设，比如我们假设模型是线性回归，还是多项式回归，以及其阶数的选择；2）误差函数定义，比如我们假设误差函数是均方误差，还是交叉熵；3）参数求解，比如使用正规方程，还是梯度下降等。 这篇文章主要讨论模型的选择问题，下面以多项式回归为例进行说明">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig001.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig002.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig003.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig004.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig005.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig006.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig007.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig008.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig009.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig010.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig011.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig012.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig013.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig014.jpg">
<meta property="article:published_time" content="2020-04-17T15:49:58.000Z">
<meta property="article:modified_time" content="2020-09-28T07:27:38.000Z">
<meta property="article:author" content="tengweitw">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig001.jpg">

<link rel="canonical" href="http://tengweitw.com/2020/04/17/[20200417]/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【图解例说机器学习】模型选择：偏差与方差 (Bias vs. Variance) | 青峰碧陋室</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">青峰碧陋室</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://tengweitw.com/2020/04/17/%5B20200417%5D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/WeiTeng.jpg">
      <meta itemprop="name" content="tengweitw">
      <meta itemprop="description" content="与有肝胆人共事，从无字句处读书。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="青峰碧陋室">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【图解例说机器学习】模型选择：偏差与方差 (Bias vs. Variance)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-17 23:49:58" itemprop="dateCreated datePublished" datetime="2020-04-17T23:49:58+08:00">2020-04-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-28 15:27:38" itemprop="dateModified" datetime="2020-09-28T15:27:38+08:00">2020-09-28</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>24 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>机器学习的过程大致分为三步：1）模型假设，比如我们假设模型是线性回归，还是多项式回归，以及其阶数的选择；2）误差函数定义，比如我们假设误差函数是均方误差，还是交叉熵；3）参数求解，比如使用正规方程，还是梯度下降等。</p>
<p>这篇文章主要讨论模型的选择问题，下面以多项式回归为例进行说明</p>
<hr>
<a id="more"></a>
<h2 id="一个例子：多项式回归中的阶数选择"><a href="#一个例子：多项式回归中的阶数选择" class="headerlink" title="一个例子：多项式回归中的阶数选择"></a>一个例子：多项式回归中的阶数选择</h2><p>在前面的文章【图解例说机器学习】线性回归中，我们定义了广义的线性回归模型，其表达式为：</p>
<script type="math/tex; mode=display">
\hat y=\omega_0+\sum\limits_{j=1}^{M}\omega_j\phi_j(\mathrm x)=\omega_0+\mathrm w^{\mathrm T}\phi(\mathrm x)\tag{1}</script><p>当$D=1,\phi_j(\mathrm x)=x^j$时，公式(1)可以表示为：</p>
<script type="math/tex; mode=display">
\hat y=\omega_0+\omega_1x+\omega_2x^2+\cdots+\omega_Mx^M\tag{2}</script><p>此时，线性回归就变成了$M$阶多项式回归。</p>
<p>当$M$及误差函数给定时，我们就可以通过梯度下降法求解得到$\mathrm w$。但是，$M$的选择对预测的结果影响较大。从公式可以看出$M$越大，模型越复杂，其函数表达式集合包含了$M$取值较小的情况。从这种角度来看，$M$取值越大越好。但是，一般来说训练数据较少，当$M$取值较大时，复杂的模型会过度学习训练数据间的关系，导致其泛化能力较差。</p>
<hr>
<p>这里我们通过一个实例来形象化$M$对算法的影响。这里我们假设实际的函数表达式为</p>
<script type="math/tex; mode=display">
y=\sin(2\pi x)+\epsilon\tag{3}</script><p>其中，$\epsilon$是一个高斯误差值。通过公式(3)我们产生10个样例点$(x_i,y_i)$。在给定不同$M$值时，我们使用正规方程法或梯度下降法可以得到最佳的函数表达式。在这里，我们采用正规方程法 (见【图解例说机器学习】线性回归中公式(12))，得到最优参数：</p>
<script type="math/tex; mode=display">
\mathrm{\bar w}=[\bar\phi^{\mathrm T}(\mathrm X)\bar\phi(\mathrm X)]^{-1}\bar\phi^{\mathrm T}(\mathrm X)\mathrm y\tag{4}</script><p>其中，这里的$\bar{\phi}^{\mathrm T}(\mathrm X)$根据公式(2)和【图解例说机器学习】线性回归中的公式(12)可得</p>
<script type="math/tex; mode=display">
\bar\phi(\mathrm X)=
\left\{\begin{matrix}
   \phi_0(\mathrm x_1) & \phi_1(\mathrm x_1) & \cdots & \phi_M(\mathrm x_1)\\
   \phi_0(\mathrm x_2) & \phi_1(\mathrm x_2) & \cdots & \phi_M(\mathrm x_2)\\
   \vdots & \vdots & \cdots &\vdots \\
   \phi_0(\mathrm x_N) & \phi_1(\mathrm x_N) & \cdots & \phi_M(\mathrm x_N)
  \end{matrix} 
  \right\}=
  \left\{\begin{matrix}
   1 & \mathrm x_1^1 & \cdots & \mathrm x_1^{M}\\
   1 & \mathrm x_2^1 & \cdots & \mathrm x_2^{M}\\
   \vdots & \vdots & \cdots &\vdots \\
   1 & \mathrm x_N & \cdots &\mathrm x_N^M
  \end{matrix} 
  \right\}\tag{5}</script><p>利用正规方程法，即公式(5)，我们可以得到如下$M$取不同值时的函数表达式：</p>
<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig001.jpg"  >图1 </center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig002.jpg"  >图2 </center></td>
    </tr>
</table>

<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig003.jpg"  >图3 </center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig004.jpg"  >图4 </center></td>
    </tr>
</table>

<hr>
<p>图1-图4表明，随着$M$的增大，函数图像对训练样本的拟合越来越好，即训练误差越来越小。但是很明显图3的图像与原始的正弦函数图像最相似，即预测误差最小。 下表给出了图1-图4对应的最优的$\mathrm w$ 的取值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>w</th>
<th style="text-align:center">$M=0$</th>
<th style="text-align:center">$M=1$</th>
<th style="text-align:center">$M=3$</th>
<th style="text-align:center">$M=9$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\omega_0$</td>
<td style="text-align:center">$-0.0379$</td>
<td style="text-align:center">$0.8309$</td>
<td style="text-align:center">$-0.2655$</td>
<td style="text-align:center">$-6.5887*10^{-2}$</td>
</tr>
<tr>
<td>$\omega_1$</td>
<td style="text-align:center"></td>
<td style="text-align:center">$-1.9631$</td>
<td style="text-align:center">$13.1817$</td>
<td style="text-align:center">$-1.9234*10^1$</td>
</tr>
<tr>
<td>$\omega_2$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$-38.3154$</td>
<td style="text-align:center">$5.2109*10^2$</td>
</tr>
<tr>
<td>$\omega_3$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$25.9214$</td>
<td style="text-align:center">$-3.8321*10^3$</td>
</tr>
<tr>
<td>$\omega_4$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$1.3080*10^4$</td>
</tr>
<tr>
<td>$\omega_5$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$-2.1917*10^4$</td>
</tr>
<tr>
<td>$\omega_6$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$1.2754*10^4$</td>
</tr>
<tr>
<td>$\omega_7$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$1.1027*10^4$</td>
</tr>
<tr>
<td>$\omega_8$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$-1.8864*10^4$</td>
</tr>
<tr>
<td>$\omega_9$</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">$7.2725*10^3$</td>
</tr>
</tbody>
</table>
</div>
<p>机器学习的目的就是选取最优的$M$值，最小化预测误差。但是实际值中，预测误差是在算法之后才能得到的(不然的话，预测有什么用)，我们都是通过验证误差来模拟预测误差。也就是说，我们一般把已经标记的数据集分为训练集和验证集，通过训练集来得到给定不同$M$时最小验证误差，从而选择最佳的$M$。图5和图6给出了$M$取值不同情况下的训练误差和验证误差：</p>
<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig005.jpg"  >图5 10个训练样例</center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig006.jpg"  >图6 100个训练样例</center></td>
    </tr>
</table>


<p>在图5和图6中，训练样例和验证样例都是由公式(3)给出，但是图5只有10个训练样例，图6有100个训练样例，验证样例都为100。从图中可知，训练误差都是随$M$增加而下降。图5中，当训练样例为10个时，此时我们可以选择$M=3$或者$M=6$得到较小的验证误差。当训练样例足够多时，如图6所示，此时$M$越大，验证误差越好。</p>
<p>根据上述讨论，我们可以总结如下：</p>
<p>当训练样例较少时，我们需要选择合适的模型的复杂度，即这里的$M$值；当训练样例较多时，我们选择的模型越复杂越好，即选择较大的$M$值。</p>
<hr>
<h4 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h4><p>当训练数据较少，而模型较为复杂时，容易出现过拟合。如在图1-图6中，只有$10$ 个训练数据，当$M=9$时，误差变大，这时出现过拟合现象。因此，我们可以通过增加训练数据和正则化来防止过拟合。</p>
<h6 id="增加训练数据"><a href="#增加训练数据" class="headerlink" title="增加训练数据"></a>增加训练数据</h6><p>图7和图8给出了，在$M=9$情况下，不同训练样例对函数表达式的模拟情况。可见，当训练样例较多时，得到的模型与原始模型(正选函数)更接近。</p>
<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig007.jpg"  >图7 50个训练样例</center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig008.jpg"  >图8 100个训练样例</center></td>
    </tr>
</table>



<h6 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h6><p>从上面的表格中可以看出，当过拟合时($M=9$)，输入变量$\mathrm x$的系数$\mathrm{w}$的系数变得特别大.此时，当$\mathrm{x}$变动十分小时，输出$\hat y$也变得很大，这就导致了预测时误差变大。此时，我们可以对误差函数加入<strong>惩罚项</strong>，来限制$\mathrm{w}$的取值：</p>
<script type="math/tex; mode=display">
E=\sum\limits_{i=1}^N{(\hat y_i-y_i)^2}+\frac{\lambda}{2}\lvert\mathrm{w}\rvert^2\tag{6}</script><p>公式(6)中的$\lambda$可以自己调节来选取合适的值。</p>
<p>同样地，我们可以使用正规方程来使得新的误差函数(6)最小。此时的解析解可以表示为：</p>
<script type="math/tex; mode=display">
\mathrm{\bar w}=[\bar\phi^{\mathrm T}(\mathrm X)\bar\phi(\mathrm X)+\lambda\mathrm I_0]^{-1}\bar\phi^{\mathrm T}(\mathrm X)\mathrm y\tag{7}</script><p>其中$\mathrm I_0$是一个$(M+1)\times(M+1)$的对角矩阵，且第一个对角元素为0(因为我们一般在正则项中不考虑$\omega_0$，见P10, <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/" target="_blank" rel="noopener">PRML</a>)，其他对角元素为$1$。</p>
<p>注意：公式(7)可以对$E$求导，使其为零得到，这里就不详细推导。</p>
<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig009.jpg"  >图9 </center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig010.jpg"  >图10 </center></td>
    </tr>
</table>

<hr>
<p>图9和图10给出了当测试样例为10，$\lambda$取不同值的函数拟合情况。可以看出，$\lambda$越小，对训练样例的拟合越好，即训练误差越小，但是此时图像与原始的正弦函数差别较大。当$\lambda=0$时，即不考虑正则化，此时对应的是图4。图9和图10说明，我们可以通过加入正则项来避免过拟合的情况。</p>
<hr>
<h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><h4 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h4><p>机器学习的目的就是最小化误差。一般采用的误差，如线性回归的平方和误差，逻辑回归的交叉熵误差。这些误差都是假定训练样例的权重一样，但是实际中，每个样例出现的概率是不同的。因此，我们这里定义一个平均误差函数:</p>
<script type="math/tex; mode=display">
\mathbb E[E]=\int\int E(\hat y,y)p(\mathrm x,y)d\mathrm xdy\tag{8}</script><p>这里的$E(\hat y,y)$就是我们常用的误差函数,如下：</p>
<script type="math/tex; mode=display">
E(\hat y, y)=(\hat y_i-y_i)^2\tag{9}</script><script type="math/tex; mode=display">
E(\hat y, y)=-[y_i\log{\hat y_i}+(1-y_i)\log{(1-\hat y_i)}]\tag{10}</script><p>可见，公式(8)是一个广义的误差函数。这里我们平方和误差函数为例，将公式(9)带入公式(8)中，我们有</p>
<script type="math/tex; mode=display">
\mathbb E[E]=\int\int (\hat y-y)^2p(\mathrm x,y)d\mathrm xdy\tag{11}</script><p>通过求导，令导数为0，我们可以得到最佳的函数表达式：</p>
<script type="math/tex; mode=display">
\frac{\partial\mathbb E(E)}{\partial\hat y}=2\int(\hat y-y)p(\mathrm x,y)dy=0\tag{12}</script><script type="math/tex; mode=display">
\hat y=\frac{\int yp(\mathrm x,y)dy}{p(\mathrm x)}=\int yp(y\mid\mathrm x)dy=\mathbb E_y[y\mid\mathrm x]=y^\star\tag{13}</script><p>根据公式(13)我们可以重写(11)</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb E(E)&=\int\int(\hat y-y^\star+y^\star-y)^2p(\mathrm x,y)d\mathrm xdy\nonumber\\
            &=\int\int[(\hat y-y^\star)^2+2(\hat y-y^\star)(y^\star-y)+(y^\star-y)^2]d\mathrm xdy\nonumber\\
            &=\int(\hat y-y^\star)^2p(\mathrm x)d\mathrm x+\int(y^\star-y)^2p(\mathrm x)d\mathrm x\tag{14}
\end{align}</script><p>公式(14)中的第二项与我们要求的函数表达式$\hat y$没有关系。因此，当我们得到最优的函数表达式($\hat y=y^\star$)，即公式第一项为0，第二项即为我们得到的最小误差值。然而，由于训练数据有限(一般假定训练数据集为$\mathcal D={\mathrm x_i\mid i=1,2,\cdots,N}$)，得到最优解($\hat y=y^\star=\mathbb E(y\mid\mathrm x)$)一般是比较困难的。但我们有充足的训练数据$\mathrm x$, 我们理论上可以得到条件期望$\mathbb E(y\mid\mathrm x)=\int yp(y\mid\mathrm x)dy$, 也就是最优的函数表达式$\hat y$。</p>
<p>真实的$y$ 与$\mathrm x$的关系由$p(\mathrm x,y)$决定，假定由$p(\mathrm x,y)$产生很多不同训练数据集 $\mathcal D$ 。对于每一个数据集$\mathcal D$, 我们都能通过机器学习算法得到一个函数表达式$\hat y<em>{\mathcal D}$。那么，我们需要在所有可能的训练数据集来评价$\hat y</em>{\mathcal D}$的好坏，即我们需要计算$\hat y_{\mathcal D}$在所以训练集上的平均误差。那么公式(14)可以写成：</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb E(E)&=\int\mathbb E_{\mathcal D}[(\hat y-y^\star)^2]p(\mathrm x)d\mathrm x+\int(y^\star-y)^2p(\mathrm x)d\mathrm x\nonumber\\
&=\int\{(\mathbb E_{\mathcal D}[\hat y_{\mathcal D}]-y^\star)^2+\mathbb E_{\mathcal D}[(\hat y_{\mathcal D}-\mathbb E_{\mathcal D}[\hat y_{\mathcal D}])^2]\}p(\mathrm x)d\mathrm x+\int(y^\star-y)^2p(\mathrm x)d\mathrm x\nonumber\\
&=\underbrace{\int(\mathbb E_{\mathcal D}[\hat y_{\mathcal D}]-y^\star)^2p(\mathrm x)d\mathrm x}_{(bias)^2}+\underbrace{\int\mathbb E_{\mathcal D}[(\hat y_{\mathcal D}-\mathbb E_{\mathcal D}[\hat y_{\mathcal D}])^2]p(\mathrm x)d\mathrm x}_{variance}+\underbrace{\int(y^\star-y)^2p(\mathrm x)d\mathrm x}_{noise}\tag{15}
\end{align}</script><p>可见，误差由bias (偏差)，variance (方差)，和noise (噪声)三部分组成。其中，bias和variance都和我们的模型选择$\hat y<em>{\mathcal D}$ 有关。第三项noise可以表示为$\int(\mathbb E</em>{\mathcal D}[y\mid\mathrm x]-y)^2p(\mathrm x)d\mathrm x$, 即可以看成是训练数据自身的特征：$y$的方差。对于公式(15)，我们可以计算各部分值如下：</p>
<ul>
<li><p>$\mathbb E<em>{\mathcal D}[\hat y</em>{\mathcal D}]$ 指的是对$\hat y$在$L$个数据集上求平均值，那么我们有：</p>
<script type="math/tex; mode=display">
\mathbb E_{\mathcal D}[\hat y_{\mathcal D}(\mathrm x)]=\frac{1}{L}\sum\limits_{l=1}^{L}\hat y_l(\mathrm x)\tag{16}</script></li>
<li><p>将公式(16)带入公式(15)中，我们有：</p>
</li>
</ul>
<script type="math/tex; mode=display">
(bias)^2=\frac{1}{N}\sum\limits_{i=1}^{N}\{\mathbb E_{\mathcal D}[\hat y_{\mathcal D}(\mathrm x_i)]-y^\star(\mathrm x_i)\}^2\tag{17}</script><script type="math/tex; mode=display">
variance=\frac{1}{N}\sum\limits_{i=1}^{N}\frac{1}{L}\sum\limits_{l=1}^{L}\{\hat y_l(\mathrm x_i)-\mathbb E_{\mathcal D}[\hat y_{\mathcal D}(\mathrm x_i)]\}^2\tag{18}</script><p>注意：在计算公式(15)的积分项$\int p(\mathrm x)d\mathrm x$ 时，我们采用的是将所有的 $\mathrm x$ 所得到的结果加和求平均，即$\sum/N$。因为这里我们假定所有训练样例都是均匀采样的。</p>
<p>一般来说，模型($\hat y(\mathrm x)$)越复杂，偏差越小，方差越大。因为模型越复杂，对于我们训练样例集的每一个样例$\mathrm x_i$的拟合较好，也就是说$\hat y_l(\mathrm x_i)$与$y^\star(\mathrm x_i)$比较接近，即公式(17)的值较小(偏差较小)；而此时，不同训练样例集产生的$\hat y_l(\mathrm x_i)$之间值存在较大波动，即公式(18)的值较大(方差较大)。换句话说，公式(17)，即偏差，针对的是$\hat y_l(\mathrm x_i)$与真实函数$y^\star(\mathrm x_i)$之间的误差；而公式(18)，即方差，针对的是不同数据集所得到的函数$\hat y_l(\mathrm x_i)$之间的误差。</p>
<hr>
<h4 id="偏差与方差的折中关系"><a href="#偏差与方差的折中关系" class="headerlink" title="偏差与方差的折中关系"></a>偏差与方差的折中关系</h4><p>我们通过函数表达式(3)来产生$L=100$个训练数据集，每个数据集包含$N=30$个训练样例。那么此时，我们有$y^\star=\sin(2\pi\mathrm x)$且此时公式(15)的第三项noise可以由$\epsilon$的分布求出。我们假定用$M=9$的多项式函数(2)来作为$\hat y$的表达式。当正则项的系数分别为$\lambda=10^{-3},10^{-12}$时，我们可以分别得到图11-12，和图13-14：</p>
<table> 
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig011.jpg"  >图11 </center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig012.jpg"  >图12 </center></td>
    </tr>
</table>

<table>
    <tr>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig013.jpg"  >图13 </center></td>
        <td ><center><img src="https://cdn.jsdelivr.net/gh/tengweitw/FigureBed@latest/20200417/Mode_selection_fig014.jpg"  >图14 </center></td>
    </tr>
</table>

<p>图11和图13表示的是在这100个训练数据集下$\hat y$关于$\mathrm x$ 的函数图像。图12和图14表示的是在左图100条函数取平均情况下的函数图像，其中红色曲线是我们最优的函数$y^\star$。左图可以反映各个函数表达式间的差别，公式(18)， 即方差，右图表示的是预测函数$\mathbb E<em>{\mathcal D}[\hat y</em>{\mathcal D}]$与最优函数$y^\star$的差别，公式(17)，即偏差。通过图11-14，我们可以看出偏差与方差的折中关系。</p>
<hr>
<p>经过上面的分析，我们可以看出误差主要由偏差、方差和噪声组成，并从中可以看出模型的选择(e.g.,这里$M,\lambda$的选择)对误差的本质影响，从而指导模型的选择。由公式(16)-(18)可以看出，误差的分析是建立在很多数据集上的统计平均值。但是在实际中，训练数据集很少。当我们有很多的数据集，我们可以把它们看出一个大的数据集，这样我们就可以防止过拟合现象(见图7-8)。</p>
<hr>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>下面我们给出图1-图14的python源代码。注意，在运行代码时，可以自行调整自变量$M,N,\lambda$等。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        图1-4的python源代码：
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2020/4/16 23:40</span></span><br><span class="line"><span class="comment"># @Author : tengweitw</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the format of labels</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LabelFormat</span><span class="params">(plt)</span>:</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    plt.tick_params(labelsize=<span class="number">14</span>)</span><br><span class="line">    labels = ax.get_xticklabels() + ax.get_yticklabels()</span><br><span class="line">    [label.set_fontname(<span class="string">'Times New Roman'</span>) <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    font = &#123;<span class="string">'family'</span>: <span class="string">'Times New Roman'</span>,</span><br><span class="line">            <span class="string">'weight'</span>: <span class="string">'normal'</span>,</span><br><span class="line">            <span class="string">'size'</span>: <span class="number">16</span>,</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="keyword">return</span> font</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_regression_normal_equation</span><span class="params">(train_data, train_target, test_data, test_target)</span>:</span></span><br><span class="line">    <span class="comment"># the 1st column is 1 i.e., x_0=1</span></span><br><span class="line">    X = np.ones([np.size(train_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_test = np.ones([np.size(test_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Here to change M !!!!!!!</span></span><br><span class="line">    M = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, M + <span class="number">1</span>):</span><br><span class="line">        temp = train_data ** i</span><br><span class="line">        temp_test = test_data ** i</span><br><span class="line">        X = np.concatenate((X, temp), axis=<span class="number">1</span>)</span><br><span class="line">        X_test = np.concatenate((X_test, temp_test), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># X is a 10*M-dim matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normal equation</span></span><br><span class="line">    w_bar = np.matmul(np.linalg.pinv(np.matmul(X.T, X)), np.matmul(X.T, train_target))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training Error</span></span><br><span class="line">    y_predict_train = np.matmul(X, w_bar)</span><br><span class="line">    E_train = np.linalg.norm(y_predict_train - train_target) / len(y_predict_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predicting</span></span><br><span class="line">    y_predict_test = np.matmul(X_test, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prediction Error</span></span><br><span class="line">    E_test = np.linalg.norm(y_predict_test - test_target) / len(y_predict_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predict_test, E_train, E_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># keep the same random training data</span></span><br><span class="line">    seed_num = <span class="number">100</span></span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    <span class="comment"># 10 training data</span></span><br><span class="line">    train_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    train_data = np.sort(train_data, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    train_target = np.sin(<span class="number">2</span> * np.pi * train_data) + <span class="number">0.1</span> * np.random.randn(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    test_data = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    test_target = np.sin(<span class="number">2</span> * np.pi * test_data) + <span class="number">0.01</span> * np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y_predict_test, E_train, E_test = Polynomial_regression_normal_equation(train_data, train_target, test_data,</span><br><span class="line">                                                                            test_target)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(train_data, train_target, <span class="string">'ro'</span>)</span><br><span class="line">    plt.plot(test_data, y_predict_test, <span class="string">'b-'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>, font)</span><br><span class="line">    plt.legend([<span class="string">'Training target'</span>, <span class="string">'Predicted target,M=2'</span>])</span><br><span class="line">    plt.ylim([<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
    </div>
</div>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        图5-8的python源代码：
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2020/4/18 11:56</span></span><br><span class="line"><span class="comment"># @Author : tengweitw</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the format of labels</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LabelFormat</span><span class="params">(plt)</span>:</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    plt.tick_params(labelsize=<span class="number">14</span>)</span><br><span class="line">    labels = ax.get_xticklabels() + ax.get_yticklabels()</span><br><span class="line">    [label.set_fontname(<span class="string">'Times New Roman'</span>) <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    font = &#123;<span class="string">'family'</span>: <span class="string">'Times New Roman'</span>,</span><br><span class="line">            <span class="string">'weight'</span>: <span class="string">'normal'</span>,</span><br><span class="line">            <span class="string">'size'</span>: <span class="number">16</span>,</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="keyword">return</span> font</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_regression_normal_equation</span><span class="params">(train_data, train_target, cv_data, cv_target,test_data,M)</span>:</span></span><br><span class="line">    <span class="comment"># the 1st column is 1 i.e., x_0=1</span></span><br><span class="line">    X = np.ones([np.size(train_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_cv = np.ones([np.size(cv_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_test = np.ones([np.size(test_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, M + <span class="number">1</span>):</span><br><span class="line">        temp = train_data ** i</span><br><span class="line">        temp_cv = cv_data ** i</span><br><span class="line">        temp_test = test_data ** i</span><br><span class="line">        X = np.concatenate((X, temp), axis=<span class="number">1</span>)</span><br><span class="line">        X_cv = np.concatenate((X_cv, temp_cv), axis=<span class="number">1</span>)</span><br><span class="line">        X_test = np.concatenate((X_test, temp_test), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># X is a 10*M-dim matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normal equation</span></span><br><span class="line">    w_bar = np.matmul(np.linalg.pinv(np.matmul(X.T, X)), np.matmul(X.T, train_target))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training Error</span></span><br><span class="line">    y_predict_train = np.matmul(X, w_bar)</span><br><span class="line">    E_train = np.linalg.norm(y_predict_train - train_target) / len(y_predict_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cross validation</span></span><br><span class="line">    y_predict_cv = np.matmul(X_cv, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prediction</span></span><br><span class="line">    y_predict_test=np.matmul(X_test, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prediction Error</span></span><br><span class="line">    E_cv = np.linalg.norm(y_predict_cv - cv_target) / len(y_predict_cv)</span><br><span class="line">    print(w_bar)</span><br><span class="line">    <span class="keyword">return</span> y_predict_test, y_predict_cv, E_train, E_cv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># keep the same random training data</span></span><br><span class="line">    seed_num = <span class="number">100</span></span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># training data</span></span><br><span class="line">    num_training=<span class="number">50</span></span><br><span class="line">    train_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (num_training, <span class="number">1</span>))</span><br><span class="line">    train_data = np.sort(train_data, axis=<span class="number">0</span>)</span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    train_target = np.sin(<span class="number">2</span> * np.pi * train_data) + <span class="number">0.1</span> * np.random.randn(num_training, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 100 cross validation data</span></span><br><span class="line">    num_cv=<span class="number">100</span></span><br><span class="line">    cv_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (num_cv, <span class="number">1</span>))</span><br><span class="line">    cv_data = np.sort(cv_data, axis=<span class="number">0</span>)</span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    cv_target = np.sin(<span class="number">2</span> * np.pi * cv_data) + <span class="number">0.1</span> * np.random.randn(num_cv, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># testing data</span></span><br><span class="line">    test_data = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    M=<span class="number">9</span>+<span class="number">1</span></span><br><span class="line">    E_train=np.zeros((M,<span class="number">1</span>))</span><br><span class="line">    E_cv=np.zeros((M,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># change M</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(M):</span><br><span class="line">        y_predict_test,y_predict_cv, E_train[i], E_cv[i] = Polynomial_regression_normal_equation(train_data, train_target, cv_data, cv_target,test_data, i)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(E_train, <span class="string">'r-o'</span>)</span><br><span class="line">    plt.plot(E_cv,<span class="string">'b-s'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'$M$'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'Error'</span>, font)</span><br><span class="line">    plt.legend([<span class="string">'Training Error'</span>, <span class="string">'Cross Error'</span>],loc=<span class="string">'upper center'</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(train_data, train_target, <span class="string">'ro'</span>)</span><br><span class="line">    plt.plot(test_data, y_predict_test, <span class="string">'b-'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>, font)</span><br><span class="line">    plt.legend([<span class="string">'Training target'</span>, <span class="string">'Predicted target,M=9'</span>])</span><br><span class="line">    plt.ylim([<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
    </div>
</div>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        图9-10的python源代码：
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2020/4/18 23:12</span></span><br><span class="line"><span class="comment"># @Author : tengweitw</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the format of labels</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LabelFormat</span><span class="params">(plt)</span>:</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    plt.tick_params(labelsize=<span class="number">14</span>)</span><br><span class="line">    labels = ax.get_xticklabels() + ax.get_yticklabels()</span><br><span class="line">    [label.set_fontname(<span class="string">'Times New Roman'</span>) <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    font = &#123;<span class="string">'family'</span>: <span class="string">'Times New Roman'</span>,</span><br><span class="line">            <span class="string">'weight'</span>: <span class="string">'normal'</span>,</span><br><span class="line">            <span class="string">'size'</span>: <span class="number">16</span>,</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="keyword">return</span> font</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_regression_normal_equation</span><span class="params">(train_data, train_target, cv_data, cv_target,test_data,M)</span>:</span></span><br><span class="line">    <span class="comment"># the 1st column is 1 i.e., x_0=1</span></span><br><span class="line">    X = np.ones([np.size(train_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_cv = np.ones([np.size(cv_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_test = np.ones([np.size(test_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Here to change lambda</span></span><br><span class="line">    Lambda=<span class="number">1e-12</span></span><br><span class="line">    I0= np.eye(M+<span class="number">1</span>)</span><br><span class="line">    I0[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, M + <span class="number">1</span>):</span><br><span class="line">        temp = train_data ** i</span><br><span class="line">        temp_cv = cv_data ** i</span><br><span class="line">        temp_test = test_data ** i</span><br><span class="line">        X = np.concatenate((X, temp), axis=<span class="number">1</span>)</span><br><span class="line">        X_cv = np.concatenate((X_cv, temp_cv), axis=<span class="number">1</span>)</span><br><span class="line">        X_test = np.concatenate((X_test, temp_test), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># X is a 10*M-dim matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normal equation</span></span><br><span class="line">    w_bar = np.matmul(np.linalg.pinv(np.matmul(X.T, X)+Lambda*I0), np.matmul(X.T, train_target))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training Error</span></span><br><span class="line">    y_predict_train = np.matmul(X, w_bar)</span><br><span class="line">    E_train = np.linalg.norm(y_predict_train - train_target) / len(y_predict_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cross validation</span></span><br><span class="line">    y_predict_cv = np.matmul(X_cv, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prediction</span></span><br><span class="line">    y_predict_test=np.matmul(X_test, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prediction Error</span></span><br><span class="line">    E_cv = np.linalg.norm(y_predict_cv - cv_target) / len(y_predict_cv)</span><br><span class="line">    print(w_bar)</span><br><span class="line">    <span class="keyword">return</span> y_predict_test, y_predict_cv, E_train, E_cv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># keep the same random training data</span></span><br><span class="line">    seed_num = <span class="number">100</span></span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># training data</span></span><br><span class="line">    num_training=<span class="number">10</span></span><br><span class="line">    train_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (num_training, <span class="number">1</span>))</span><br><span class="line">    train_data = np.sort(train_data, axis=<span class="number">0</span>)</span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    train_target = np.sin(<span class="number">2</span> * np.pi * train_data) + <span class="number">0.1</span> * np.random.randn(num_training, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 100 cross validation data</span></span><br><span class="line">    num_cv=<span class="number">100</span></span><br><span class="line">    cv_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (num_cv, <span class="number">1</span>))</span><br><span class="line">    cv_data = np.sort(cv_data, axis=<span class="number">0</span>)</span><br><span class="line">    np.random.seed(seed_num)</span><br><span class="line">    cv_target = np.sin(<span class="number">2</span> * np.pi * cv_data) + <span class="number">0.1</span> * np.random.randn(num_cv, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># testing data</span></span><br><span class="line">    test_data = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    M=<span class="number">9</span></span><br><span class="line"></span><br><span class="line">    y_predict_test,y_predict_cv, E_train, E_cv = Polynomial_regression_normal_equation(train_data, train_target, cv_data, cv_target,test_data, M)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(train_data, train_target, <span class="string">'ro'</span>)</span><br><span class="line">    plt.plot(test_data, y_predict_test, <span class="string">'b-'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>, font)</span><br><span class="line">    plt.legend([<span class="string">'Training target'</span>, <span class="string">'Predicted target,$M=9,\lambda=10^&#123;-12&#125;$'</span>])</span><br><span class="line">    plt.ylim([<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

    </div>
</div>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        图11-14的python源代码：
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2020/4/20 10:46</span></span><br><span class="line"><span class="comment"># @Author : tengweitw</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the format of labels</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LabelFormat</span><span class="params">(plt)</span>:</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    plt.tick_params(labelsize=<span class="number">14</span>)</span><br><span class="line">    labels = ax.get_xticklabels() + ax.get_yticklabels()</span><br><span class="line">    [label.set_fontname(<span class="string">'Times New Roman'</span>) <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    font = &#123;<span class="string">'family'</span>: <span class="string">'Times New Roman'</span>,</span><br><span class="line">            <span class="string">'weight'</span>: <span class="string">'normal'</span>,</span><br><span class="line">            <span class="string">'size'</span>: <span class="number">16</span>,</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="keyword">return</span> font</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_regression_normal_equation</span><span class="params">(train_data, train_target,test_data,M)</span>:</span></span><br><span class="line">    <span class="comment"># the 1st column is 1 i.e., x_0=1</span></span><br><span class="line">    X = np.ones([np.size(train_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    X_test = np.ones([np.size(test_data, <span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Here to change lambda</span></span><br><span class="line">    Lambda=<span class="number">1e-8</span></span><br><span class="line">    I0= np.eye(M+<span class="number">1</span>)</span><br><span class="line">    I0[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, M + <span class="number">1</span>):</span><br><span class="line">        temp = train_data ** i</span><br><span class="line">        temp_test = test_data ** i</span><br><span class="line">        X = np.concatenate((X, temp), axis=<span class="number">1</span>)</span><br><span class="line">        X_test = np.concatenate((X_test, temp_test), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># X is a 10*M-dim matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normal equation</span></span><br><span class="line">    w_bar = np.matmul(np.linalg.pinv(np.matmul(X.T, X)+Lambda*I0), np.matmul(X.T, train_target))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training Error</span></span><br><span class="line">    y_predict_train = np.matmul(X, w_bar)</span><br><span class="line">    E_train = np.linalg.norm(y_predict_train - train_target) / len(y_predict_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prediction</span></span><br><span class="line">    y_predict_test=np.matmul(X_test, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prediction Error</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predict_test, E_train</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># L is  the number of training data sets</span></span><br><span class="line">    L=<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># number of each training data</span></span><br><span class="line">    N=<span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># For L training datasets</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line"></span><br><span class="line">        train_data = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (N, <span class="number">1</span>))</span><br><span class="line">        train_data = np.sort(train_data, axis=<span class="number">0</span>)</span><br><span class="line">        train_target = np.sin(<span class="number">2</span> * np.pi * train_data) + <span class="number">0.1</span>* np.random.randn(N, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># testing data</span></span><br><span class="line">        test_data = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        M=<span class="number">9</span></span><br><span class="line">        y_predict_test, E_train = Polynomial_regression_normal_equation(train_data, train_target, test_data, M)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> l==<span class="number">0</span>:</span><br><span class="line">            predict_target=y_predict_test</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            predict_target=np.hstack((y_predict_test,predict_target))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plt.plot(train_data, train_target, 'ro')</span></span><br><span class="line">        plt.plot(test_data, y_predict_test, <span class="string">'b-'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>, font)</span><br><span class="line">    plt.legend([ <span class="string">'Predicted target,$M=9,\lambda=10^&#123;-3&#125;$'</span>])</span><br><span class="line"></span><br><span class="line">    plt.ylim(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    predict_target_avg=np.mean(predict_target,axis=<span class="number">1</span>)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(test_data,predict_target_avg,<span class="string">'r-'</span>)</span><br><span class="line">    test_target = np.sin(<span class="number">2</span> * np.pi * test_data)</span><br><span class="line">    plt.plot(test_data,test_target,<span class="string">'b-'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the labels</span></span><br><span class="line">    font = LabelFormat(plt)</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>, font)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>, font)</span><br><span class="line">    plt.legend([<span class="string">'True model'</span>, <span class="string">'Predicted model,$M=9,\lambda=10^&#123;-3&#125;$'</span>])</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
    </div>
</div>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
    </div>

    
    
    
        <div class="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="tengweitw 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="tengweitw 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>tengweitw
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://tengweitw.com/2020/04/17/[20200417]/" title="【图解例说机器学习】模型选择：偏差与方差 (Bias vs. Variance)">http://tengweitw.com/2020/04/17/[20200417]/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/14/%5B20200414%5D/" rel="prev" title="【图解例说机器学习】逻辑回归(Logistic Regression)">
      <i class="fa fa-chevron-left"></i> 【图解例说机器学习】逻辑回归(Logistic Regression)
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/30/%5B20200430%5D/" rel="next" title="【图解例说机器学习】决策树 (Decision Tree)">
      【图解例说机器学习】决策树 (Decision Tree) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一个例子：多项式回归中的阶数选择"><span class="nav-number">1.</span> <span class="nav-text">一个例子：多项式回归中的阶数选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#防止过拟合"><span class="nav-number">1.0.1.</span> <span class="nav-text">防止过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#增加训练数据"><span class="nav-number">1.0.1.0.1.</span> <span class="nav-text">增加训练数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#正则化"><span class="nav-number">1.0.1.0.2.</span> <span class="nav-text">正则化</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差与方差"><span class="nav-number">2.</span> <span class="nav-text">偏差与方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#理论推导"><span class="nav-number">2.0.1.</span> <span class="nav-text">理论推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差的折中关系"><span class="nav-number">2.0.2.</span> <span class="nav-text">偏差与方差的折中关系</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#附录"><span class="nav-number">3.</span> <span class="nav-text">附录</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="tengweitw"
      src="/images/WeiTeng.jpg">
  <p class="site-author-name" itemprop="name">tengweitw</p>
  <div class="site-description" itemprop="description">与有肝胆人共事，从无字句处读书。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tengweitw@gmail.com" title="E-Mail → mailto:tengweitw@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5bcc8onhhan&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tengweitw</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">735k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">17:31</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.5.0
  </div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>



  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
